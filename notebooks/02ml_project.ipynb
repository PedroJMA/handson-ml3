{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. End-to-end machine learning project \n",
    "In this chapter you will work through an example project end to end. \n",
    "\n",
    "The main steps you will go through: \n",
    "1. Look at the big picture. \n",
    "2. Get the data. \n",
    "3. Discover and visualize the data to gain insights. \n",
    "4. Prepare the data for Machine Learning algorithms. \n",
    "5. Select a model and train it. \n",
    "6. Fine-tune your model. \n",
    "7. Present your solution. \n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with real data \n",
    "It is best with real world data, not artificial datasets. \n",
    "There are thousands of open datasets to choose form. \n",
    "\n",
    "In this chapter we'll use the California Housing Prices dataset from the StatLib repository. \n",
    "\n",
    "## Look at the big picture \n",
    "Your fist task is to use California census data to build a model of housing prices in the state. \n",
    "This data includes metrics such as the population, median income, and median hosing price for each block group (\"districts\") in California. \n",
    "Your model should learn from this data and be able to predict the median housing price in any district, given all the other metrics. \n",
    "\n",
    "> Since you are a well-organized data scientist, the first thing you should do is pull out your Machine Learning project checklist. \n",
    "\n",
    "### Frame the problem \n",
    "The business object is. \n",
    "How does the company expect to use and benefit from this model? \n",
    "Knowing the objective is important because it will determine how you frame the problem, which algorithms you will select, which performance measure you will use to evaluate your model, and how much effort you will spend tweaking it. \n",
    "\n",
    "Your boss answers that your model's output (a prediction of a district's median housing price) will be fed to another Machine Learning system, along with many other signals. \n",
    "This downstream system will determine whether it is worth investing in a given area or not. \n",
    "Getting this right is critical, as it directly affects revenue. \n",
    "\n",
    "> A piece of information fed to a machine Learning system is often called a **signal**, in reference Claude Shannon's information theory, which he developed at Bell Labs to improve telecommunications. His theory: you want a high signal-to-noise ratio. \n",
    "\n",
    "[See figure 2.2 A machine learning pipeline for real estate investments]\n",
    "\n",
    "> **Pipelines** A sequence of data processing components is called a data *pipeline*. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply. \n",
    "> Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph).\n",
    "> If a component breaks down, the downstream components can often continue to run normally (at least for while) by using the last output from the broken component. This makes the architecture quite robust. \n",
    "> On the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system's performance drops. \n",
    "\n",
    "The next question to ask your boss is what the current solution looks like (if any). The current situation will often give you a reference for performance, as well as insights on how to solve the problem. \n",
    "\n",
    "With all this information, you are ready to start **designing your system**. \n",
    "First, you need to **frame the problem**: \n",
    "Is it supervised, unsupervised, or reinforcement learning? \n",
    "Is it a classification task, a regression task, or something else? \n",
    "Should you use batch learning or online learning techniques? \n",
    "<!-- IMO: Supervised, regression task, batch learning -->\n",
    "\n",
    "In this example, it is clearly a typical supervised learning task, since you are given **labeled** training examples (each instance comes with the expected output, i.e., the district's median housing price). \n",
    "It is also a typical regression task, since you are asked to predict a value. \n",
    "More specifically, this is a *multiple regression* problem, since the system will use multiple features to make a prediction. \n",
    "It is a *univariate regression* problem, wince we are only trying to predict a single value for each district. \n",
    "Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine. \n",
    "\n",
    "> If the data were hugh, you could either split your batch learning work across multiple servers (using the **MapReduce technique**) or use an online learning technique. \n",
    "\n",
    "### Select a performance \n",
    "A typical performance measure for regression problems is the **Root Mean Square Error (RMSE)**.\n",
    "It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. \n",
    "\n",
    "[Equation 2.1 Root Mean Square Error (RMSE) on pp. 39]\n",
    "\n",
    "> **Notations** \n",
    "> * $m$ is the number of instances in the dataset you are measuring the RMSE on. \n",
    "> * $\\mathbf{x}^{(i)}$ is a vector of all feature values (excluding the label) of the $i^{ht}$ instance in the dataset, and $y^{(i)}$ is its label (the desired output value for that instance). \n",
    "> * $\\mathbf{X}$ is a matrix containing all the feature values (excluding labels) of all instances in the dataset. There is one row per instance, and the $i^{th}$ row is equal to the transpose of $\\mathbf{x}^{(i)}$, noted $( \\mathbf{x}^{(i)} )^T$. \n",
    "> * $h$ is yor system's prediction function, also called a *hypothesis*. When your system is given an instance's feature vector $\\mathbf{x}^{(i)}$, it outputs a predicted value $\\hat{y}^{(i)} = h(\\mathbf{x}^{(i)})$ for that instance ($\\hat{y}$ is pronounced \"y-hat\"). \n",
    "> * $RMSE (\\mathbf{X}, h)$ is the cost function measured on the set of examples using your hypothesis $h$. \n",
    "\n",
    "Even though the RMSE is generally the preferred performance measure for regression tasks, in some contexts you may prefer to use another function. \n",
    "\n",
    "For example, suppose that there are many outliers districts. \n",
    "In that case, you may consider using the **mean absolute error** (MAE, also called the average absolute deviation). \n",
    "\n",
    "[Equation 2-2. Mean absolute error (MAE)]\n",
    "\n",
    "Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. \n",
    "Various distance measures, or **norms**, are possible: \n",
    "* Computing the root of a sum of squares (RMSE) corresponds to the *Euclidean norm*. It is also called the $\\mathcal{l}_2$ norm, noted $||\\cdot||_2$ or just  $||\\cdot||$.\n",
    "* Computing the sum of absolutes (MAE) corresponds to the $\\mathcal{l}_1$, noted $||\\cdot||_1$. This is sometimes called the *Manhattan norm* because it measures the distance between two points in a city if you can only travel along orthogonal city blocks. \n",
    "* The $\\mathcal{l}_k$ norm of a vector $\\mathbf{v}$ containing $n$ elements is defined as $||\\mathbf{v}||_k$ \n",
    "* The higher the norm index, the more it focuses on large values and neglects small ones. This is why the RMSE is more sensitive to outliers than the MAE. \n",
    "\n",
    "\n",
    "### Check the assumptions \n",
    "Lastly, it is good practice to list and verify the assumptions that have been made so far (by you or others); this can help you catch serious issues early on. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data \n",
    "### Create the workspace \n",
    "\n",
    "### download the data \n",
    "### take a quick look at the data structure \n",
    "### create a test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discover and visualize the data to gain insights \n",
    "### Visualizing geographical data \n",
    "### looking for correlations \n",
    "### experimenting with attribute combinations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for machine learning algorithms\n",
    "### Data cleaning \n",
    "### Handling text and categorical attributes \n",
    "### Custom transformers \n",
    "### Feature scaling \n",
    "### Transformation pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select and train a model \n",
    "### Training and evaluation on the training set \n",
    "### Better evaluation using cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tune your model \n",
    "### Grid search \n",
    "### Randomized search \n",
    "### Ensemble methods \n",
    "### Analyze the best models and their erros \n",
    "### Evaluate your system on the test set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch, Monitor, and Maintain your system \n",
    "## Try it, out! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
